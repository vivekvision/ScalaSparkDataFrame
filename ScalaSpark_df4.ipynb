{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0178e26-76a1-4113-bfb8-9a8f46997a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@42229c38\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"TestApp\")\n",
    "                        .config(\"spark.master\", \"local\")\n",
    "                        .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8727ab6-857a-4909-a3ab-150ed235498b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Data_2006_2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56692061-0f1d-4c56-89a3-225ec1894055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(month(df(\"Date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c0cde2-6e55-4837-9963-ebcfb2323055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(year(df(\"Date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a902312-1659-4ce5-af9c-6a3ddf07fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2007| 477.8203984063745|\n",
      "|2006| 489.2697211155379|\n",
      "|2008|190.48893280632404|\n",
      "+----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\n",
       "df_avgs: org.apache.spark.sql.DataFrame = [Year: int, avg(Open): double ... 5 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.withColumn(\"Year\", year(df(\"Date\")))\n",
    "\n",
    "val df_avgs = df2.groupBy(\"Year\").mean()\n",
    "\n",
    "// average closing price for each year \n",
    "df_avgs.select($\"Year\",$\"avg(Close)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9f7d0b-6634-4831-999b-a2ff9c4f6d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Year|min(Close)|\n",
      "+----+----------+\n",
      "|2007|     292.9|\n",
      "|2006|     450.5|\n",
      "|2008|      37.7|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\n",
       "df_mins: org.apache.spark.sql.DataFrame = [Year: int, min(Open): double ... 5 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df.withColumn(\"Year\", year(df(\"Date\")))\n",
    "val df_mins = df3.groupBy(\"Year\").min()\n",
    "\n",
    "df_mins.select($\"Year\", $\"min(Close)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e489b7-2c1a-44c2-8c30-74ce03e89e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
